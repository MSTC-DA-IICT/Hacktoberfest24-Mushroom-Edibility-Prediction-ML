{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Noise Reduction\n",
    "Implement a model that can perform image noise reduction, enhancing the quality of images by reducing noise while preserving important features.\n",
    "\n",
    "### Objective\n",
    "Develop and train models that effectively reduce noise in images, improving overall clarity and visual quality.\n",
    "\n",
    "### Tasks\n",
    "- Data Cleaning & Transformation: Preprocessing the provided dataset of clean images to generate noisy images for model training.\n",
    "- Feature Engineering: Analyze noise patterns and apply transformations to optimize the dataset.\n",
    "- Encoding of images to ensure they are suitable for processing by the models, utilizing libraries such as `OpenCV` for image manipulation.\n",
    "\n",
    "### Link to the dataset\n",
    "https://drive.google.com/file/d/1yqLUYqU8_elMhrk1Ebe6p-RdHgmixuta/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Image Noise Reduction Model\n",
    "\n",
    "This notebook implements various models for reducing noise in images while preserving important features.\n",
    "\n",
    "## Table of Contents\n",
    "1. Introduction\n",
    "2. Setup\n",
    "3. Data Preprocessing\n",
    "4. Model Implementations\n",
    "    - CNN Model\n",
    "    - U-Net Model\n",
    "    - Denoising Autoencoder\n",
    "    - Non-Local Means Denoising\n",
    "5. Hyperparameter Tuning\n",
    "6. Training and Evaluation\n",
    "7. Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Introduction\n",
    "#please if someone can write a quick intro for this i have made the basics for all the models\n",
    "\n",
    "# 2. Setup\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# 3. Data Preprocessing\n",
    "def load_data(directory):\n",
    "    images = []\n",
    "    for folder in os.listdir(directory):\n",
    "        for file in os.listdir(os.path.join(directory, folder)):\n",
    "            img = cv2.imread(os.path.join(directory, folder, file))\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# Load training and validation sets\n",
    "X_train = load_data('') #'path/to/training_set\n",
    "X_val = load_data('') #path/to/validation_set\n",
    "\n",
    "def add_noise(img):\n",
    "    row, col, ch = img.shape\n",
    "    mean = 0\n",
    "    var = 0.1\n",
    "    sigma = var ** 0.5\n",
    "    gauss = np.random.normal(mean, sigma, (row, col, ch))\n",
    "    noisy = img + gauss\n",
    "    return noisy\n",
    "\n",
    "# Create noisy dataset\n",
    "X_train_noisy = np.array([add_noise(img) for img in X_train])\n",
    "X_val_noisy = np.array([add_noise(img) for img in X_val])\n",
    "\n",
    "# 4. Model Implementations\n",
    "## A. CNN Model\n",
    "def cnn_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2DTranspose(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same'))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Instantiate and train the CNN model\n",
    "cnn_input_shape = (128, 128, 3)  # Adjust as necessary\n",
    "cnn = cnn_model(cnn_input_shape)\n",
    "cnn.fit(X_train_noisy, X_train, epochs=20, batch_size=32, validation_data=(X_val_noisy, X_val))\n",
    "\n",
    "## B. U-Net Model\n",
    "def unet_model(input_shape):\n",
    "    inputs = layers.Input(input_shape)\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "    u1 = layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same')(p1)\n",
    "    u1 = layers.concatenate([u1, c1])\n",
    "    outputs = layers.Conv2D(3, (1, 1), activation='sigmoid')(u1)\n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Instantiate and train the U-Net model\n",
    "unet = unet_model(cnn_input_shape)\n",
    "unet.fit(X_train_noisy, X_train, epochs=20, batch_size=32, validation_data=(X_val_noisy, X_val))\n",
    "\n",
    "## C. Denoising Autoencoder\n",
    "def autoencoder(input_shape):\n",
    "    input_img = layers.Input(shape=input_shape)\n",
    "    encoded = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    encoded = layers.MaxPooling2D((2, 2))(encoded)\n",
    "    decoded = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    decoded = layers.UpSampling2D((2, 2))(decoded)\n",
    "    decoded = layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(decoded)\n",
    "    autoencoder = models.Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder\n",
    "\n",
    "# Instantiate and train the Autoencoder model\n",
    "autoencoder_model = autoencoder(cnn_input_shape)\n",
    "autoencoder_model.fit(X_train_noisy, X_train, epochs=20, batch_size=32, validation_data=(X_val_noisy, X_val))\n",
    "\n",
    "## D. Non-Local Means Denoising\n",
    "def non_local_means(img):\n",
    "    return cv2.fastNlMeansDenoisingColored(img, None, 10, 10, 7, 21)\n",
    "\n",
    "# Example usage\n",
    "sample_image = X_val_noisy[0]\n",
    "denoised_image = non_local_means(sample_image)\n",
    "\n",
    "# Show original and denoised images\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Noisy Image')\n",
    "plt.imshow(sample_image.astype(np.uint8))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Denoised Image')\n",
    "plt.imshow(denoised_image)\n",
    "plt.show()\n",
    "\n",
    "# 5. Hyperparameter Tuning\n",
    "# implement GridSearchCV for hyperparameter tuning as shown in the previous guidance.\n",
    "\n",
    "# 6. Training and Evaluation\n",
    "# Evaluate model performance on validation set.\n",
    "\n",
    "# 7. Conclusion\n",
    "# Summarize the findings and the performance of each model.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
